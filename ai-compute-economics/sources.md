# AI Compute Economics Data Sources

## Benchmarks

| Source | URL | What It Provides |
|--------|-----|-----------------|
| **MLPerf Training** | https://mlcommons.org/benchmarks/training/ | Standardized training performance across hardware |
| **MLPerf Inference** | https://mlcommons.org/benchmarks/inference-datacenter/ | Standardized inference throughput and latency |
| **Artificial Analysis** | https://artificialanalysis.ai/ | Independent inference speed, cost, and quality comparisons |
| **LMSYS Chatbot Arena** | https://chat.lmsys.org/ | Model quality rankings via human preference |
| **Open LLM Leaderboard** | https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard | Open model benchmark comparisons |

## Hardware Specifications

| Source | URL | What It Provides |
|--------|-----|-----------------|
| **NVIDIA Product Pages** | https://www.nvidia.com/en-us/data-center/ | GPU specs, TDP, memory config, interconnect |
| **AMD Instinct** | https://www.amd.com/en/products/accelerators/instinct.html | MI-series specs and benchmarks |
| **Google Cloud TPU** | https://cloud.google.com/tpu/docs | TPU specs, pricing, performance guides |
| **Intel Gaudi** | https://habana.ai/ | Gaudi accelerator specs and benchmarks |

## Cloud Pricing

| Source | URL | What It Provides |
|--------|-----|-----------------|
| **AWS EC2 Pricing** | https://aws.amazon.com/ec2/pricing/ | GPU instance pricing (on-demand, reserved, spot) |
| **GCP Pricing** | https://cloud.google.com/compute/gpus-pricing | GPU and TPU pricing |
| **Azure Pricing** | https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/ | GPU VM pricing |
| **CoreWeave Pricing** | https://www.coreweave.com/pricing | GPU cloud pricing |
| **Lambda Pricing** | https://lambdalabs.com/service/gpu-cloud | GPU cloud pricing |
| **Together AI Pricing** | https://www.together.ai/pricing | Inference API pricing by model |

## Research & Analysis

| Source | URL | What It Provides |
|--------|-----|-----------------|
| **Epoch AI** | https://epochai.org/ | Training compute estimates, scaling trends, parameter counts |
| **Stanford AI Index** | https://aiindex.stanford.edu/ | Annual AI progress metrics, cost trends |
| **Our World in Data (AI)** | https://ourworldindata.org/artificial-intelligence | Training compute growth, cost decline curves |
| **SemiAnalysis** | https://semianalysis.com/ | GPU economics, accelerator models, datacenter cost analysis |

## Vendor Earnings & Disclosures

| Source | Frequency | Key Datapoints |
|--------|-----------|---------------|
| **NVIDIA earnings** | Quarterly | Data center revenue, GPU shipment signals, margin |
| **AMD earnings** | Quarterly | MI-series revenue, design wins |
| **Hyperscaler earnings** | Quarterly | AI capex, GPU fleet size signals, utilization commentary |
| **CoreWeave/Lambda** | Occasional | Fleet size, committed revenue, pricing changes |

## Technical References

| Source | URL | What It Provides |
|--------|-----|-----------------|
| **vLLM docs** | https://docs.vllm.ai/ | Inference engine performance, batching strategies |
| **NVIDIA TensorRT-LLM** | https://github.com/NVIDIA/TensorRT-LLM | Optimized inference, quantization, benchmarks |
| **DeepSpeed docs** | https://www.deepspeed.ai/ | Training parallelism, ZeRO, inference optimization |
| **Hugging Face TGI** | https://huggingface.co/docs/text-generation-inference/ | Inference serving performance |
